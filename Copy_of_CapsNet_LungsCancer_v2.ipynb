{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CapsNet_LungsCancer_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gershom1/Practical-Introduction-to-python/blob/patch-1/Copy_of_CapsNet_LungsCancer_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQYMAcNKa7xu"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense\n",
        "from tensorflow.keras.layers import Reshape, Layer, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import matplotlib.pylab as plt\n",
        "import cv2   \n",
        "import os\n",
        "from google.colab.patches import cv2_imshow # for image display\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "ZQ6fvFoEbMlM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee19fb1d-391e-4207-f6c9-564a7dd20cdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('/content/drive/My Drive/GershomProject/label.csv') # read the dataset to a pandas data frame\n",
        "df_train.head() # check out the details of the dataset"
      ],
      "metadata": {
        "id": "x-Mpc0EycHqa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "742d4886-bfab-445f-e723-427c5be3a3f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    image_id  class\n",
              "0  00000.jpg      0\n",
              "1  00001.jpg      0\n",
              "2  00002.jpg      0\n",
              "3  00003.jpg      0\n",
              "4  00004.jpg      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f892c0be-da19-44e5-87b9-4fade6917622\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00000.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00001.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00002.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00003.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00004.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f892c0be-da19-44e5-87b9-4fade6917622')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f892c0be-da19-44e5-87b9-4fade6917622 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f892c0be-da19-44e5-87b9-4fade6917622');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_labels(inputPath):\n",
        "    # initialize the list of column names in the CSV file and then load it using Pandas\n",
        "    col_list = [\"image_id\", \"class\"]\n",
        "    df = pd.read_csv(inputPath, usecols=col_list)\n",
        "    return df\n",
        "\n",
        "# this function is for the sort function in load_gaze_images() function below\n",
        "def getint(name):\n",
        "    basename, _ = name.split('.')\n",
        "    return int(basename)\n",
        "\n",
        "def read_image(fname, IMAGE_DIMS):\n",
        "    # read each image in colored\n",
        "    img = cv2.imread(fname) \n",
        "    \n",
        "    # resize each image\n",
        "    f_h, f_w = int(IMAGE_DIMS[0]), int(IMAGE_DIMS[1]) # height and width\n",
        "    img = cv2.resize(img, (f_h, f_w))\n",
        "    \n",
        "    # convert image to RGB for ease of training\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img_to_array(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "# process and dataset\n",
        "def load_imageAndLabel(dir, IMAGE_DIMS, dataset_name, label_file):\n",
        "    batch_data = []\n",
        "    classes = []\n",
        "    \n",
        "    # get the path for the labels. \n",
        "    label_path = os.path.join(dir, label_file)\n",
        "    df_label = load_labels(label_path)\n",
        "    \n",
        "    # get the path for all the files in the current sub directory\n",
        "    img_path = os.path.join(dir, dataset_name) \n",
        "    \n",
        "    # get the file names for all the files\n",
        "    files = os.listdir(img_path)\n",
        "\n",
        "    n_img = len(files)  # get the total number of images\n",
        "    for i in tqdm(range(n_img)): \n",
        "        # get the file path \n",
        "        fname = os.path.join(img_path, files[i])\n",
        "        print(fname)\n",
        "        idx = df_label[df_label[\"image_id\"] == files[i]].index.values # get the index of the current image\n",
        "        clas = df_label[\"class\"].values[idx]\n",
        "        classes.append(clas)\n",
        "        img = read_image(fname, IMAGE_DIMS)\n",
        "        batch_data.append(img) # create batch data\n",
        "    one = classes.count(0)\n",
        "    two = classes.count(1)\n",
        "    three = classes.count(2)\n",
        "    norm = classes.count(3)\n",
        "    return np.array(batch_data), np.array(classes)"
      ],
      "metadata": {
        "id": "xRQV41vAcUy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "img_dir = r'/content/drive/My Drive/GershomProject/LungsCancerDataset/'\n",
        "files = os.listdir(img_dir)"
      ],
      "metadata": {
        "id": "l1EVx6_NgDAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = [224, 224, 3]\n",
        "dimA, dimB, n_channel = input_shape\n",
        "tot_n_pixels = dimA * dimB * n_channel\n",
        "train_dir = '/content/drive/My Drive/GershomProject/'\n",
        "#img_dir = r'/content/drive/My Drive/GershomProject/LungsDataset/'\n",
        "dataset_name = \"LungsCancerDataset\" \n",
        "label_file = \"label.csv\"\n",
        "n_class = 4\n",
        "n_routing = 3\n",
        "\n",
        "print(\"[INFO] loading training and test data...\")\n",
        "images, labels = load_imageAndLabel(train_dir, input_shape, dataset_name, label_file)"
      ],
      "metadata": {
        "id": "hr1rLJyAfGM6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "172284f1-f62f-4a7c-e87e-5ac8112b910c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading training and test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/GershomProject/LungsCancerDataset/normal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-fd914fd9780e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] loading training and test data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_imageAndLabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-1206eb74ab8b>\u001b[0m in \u001b[0;36mload_imageAndLabel\u001b[0;34m(dir, IMAGE_DIMS, dataset_name, label_file)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mclas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_DIMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mbatch_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# create batch data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-1206eb74ab8b>\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(fname, IMAGE_DIMS)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# resize each image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mf_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_DIMS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_DIMS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# height and width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# convert image to RGB for ease of training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# partition the data into training and testing splits using 80% of\n",
        "# the data for training and the remaining 20% for testing\n",
        "split = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "(x_train, x_test, y_train, y_test) = split"
      ],
      "metadata": {
        "id": "l2oDJ73IjaZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale or normalize the dataset\n",
        "print(\"[INFO] scaling the dataset to range [0,1]...\")\n",
        "x_train = x_train.reshape(-1, dimA, dimB, n_channel).astype('float32') / 255.0 # reshape and normalize the training set\n",
        "x_test = x_test.reshape(-1, dimA, dimB, n_channel).astype('float32') / 255.0 # reshape and normalize the test set\n",
        "\n",
        "# one hot encode target values\n",
        "y_train = to_categorical(y_train.astype('float32'))\n",
        "y_test = to_categorical(y_test.astype('float32'))"
      ],
      "metadata": {
        "id": "rvPmmIV0jlQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# join the train and test dataset\n",
        "X = np.concatenate((x_train, x_test), axis=0)\n",
        "Y = np.concatenate((y_train, y_test), axis=0)"
      ],
      "metadata": {
        "id": "kkowW32akT2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pass the reshaped output through a non-linear squash function to ensure that the length of each vector is between 0 and 1\n",
        "def squash(output_vector, axis=-1):\n",
        "    # take norm of input vectors\n",
        "    norm = tf.reduce_sum(tf.square(output_vector), axis, keepdims=True) \n",
        "\n",
        "    # use the formula for non-linear function to return squashed output\n",
        "    return output_vector * norm / ((1 + norm) * tf.sqrt(norm + 1.0e-10))"
      ],
      "metadata": {
        "id": "B0Y1ghzgkYK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input from the digit capsule is masked with the original or actual label. This is done to ensure that the decoder is trained on the correct digit capsule.\n",
        "class MaskingLayer(Layer):\n",
        "    def call(self, inputs, **kwargs):\n",
        "        input, mask = inputs\n",
        "        return K.batch_dot(input, mask, 1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        *_, output_shape = input_shape[0]\n",
        "        return (None, output_shape)"
      ],
      "metadata": {
        "id": "j9wL599pkbaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the primary capsule layer performs the following: conv, reshape, and lambda\n",
        "def PrimaryCapsule(n_vector, n_channel, n_kernel_size, n_stride, padding='valid'):\n",
        "    def builder(inputs):\n",
        "        output = Conv2D(filters=n_vector * n_channel, kernel_size=n_kernel_size, strides=n_stride, padding=padding)(inputs)\n",
        "        output = Reshape( target_shape=[-1, n_vector], name='primary_capsule_reshape')(output) # reshape to the required number of capsules and vector per capsule\n",
        "        return Lambda(squash, name='primary_capsule_squash')(output) # squash the reshaped output to make length of vector b/w 0 and 1\n",
        "    return builder"
      ],
      "metadata": {
        "id": "5Znu4Jjlke-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DigitCapsuleLayer(Layer):\n",
        "    def __init__(self, n_capsule, n_vec, n_routing, **kwargs):\n",
        "        super(DigitCapsuleLayer, self).__init__(**kwargs)\n",
        "        self.n_capsule = n_capsule\n",
        "        self.n_vector = n_vec\n",
        "        self.n_routing = n_routing\n",
        "        self.kernel_initializer = initializers.get('he_normal')\n",
        "        self.bias_initializer = initializers.get('zeros')\n",
        "\n",
        "    def build(self, input_shape): # input_shape is a 4D tensor\n",
        "        _, self.input_n_capsule, self.input_n_vector, *_ = input_shape\n",
        "        #self.W = self.add_weight(shape=[self.input_n_capsule, self.n_capsule, self.input_n_vector, self.n_vector], initializer=self.kernel_initializer, name='W')\n",
        "        #self.W = self.add_weight(shape=[self.n_capsule, self.input_n_capsule, self.input_n_vector, self.n_vector], initializer=self.kernel_initializer, name='W')\n",
        "        self.W = self.add_weight(shape=[self.n_capsule, self.input_n_capsule, self.n_vector, self.input_n_vector], initializer=self.kernel_initializer, name='W')\n",
        "        self.bias = self.add_weight(shape=[1, self.input_n_capsule, self.n_capsule, 1, 1], initializer=self.bias_initializer, name='bias', trainable=False)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # Expand the input in axis=1, tile in that axis to num_capsule, and \n",
        "        # expands another axis at the end to prepare the multiplication with W.\n",
        "        #  inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
        "        #  inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
        "        #  inputs_tiled.shape=[None, num_capsule, input_num_capsule, \n",
        "        #                            input_dim_capsule, 1]\n",
        "        inputs_expand = tf.expand_dims(inputs, 1)\n",
        "        inputs_tiled  = tf.tile(inputs_expand, [1, self.n_capsule, 1, 1])\n",
        "        inputs_tiled  = tf.expand_dims(inputs_tiled, 4)\n",
        "\n",
        "        # Compute `W * inputs` by scanning inputs_tiled on dimension 0 (map_fn).\n",
        "        # - Use matmul (without transposing any element). Note the order!\n",
        "        # Thus:\n",
        "        #  x.shape=[num_capsule, input_num_capsule, input_dim_capsule, 1]\n",
        "        #  W.shape=[num_capsule, input_num_capsule, dim_capsule,input_dim_capsule]\n",
        "        # Regard the first two dimensions as `batch` dimension,\n",
        "        # then matmul: [dim_capsule, input_dim_capsule] x [input_dim_capsule, 1]-> \n",
        "        #              [dim_capsule, 1].\n",
        "        #  inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule, 1]\n",
        "        \n",
        "        inputs_hat = tf.map_fn(lambda x: tf.matmul(self.W, x), elems=inputs_tiled)     \n",
        "\n",
        "        # Begin: Routing algorithm ----------------------------------------------#\n",
        "        # The prior for coupling coefficient, initialized as zeros.\n",
        "        #  b.shape = [None, self.num_capsule, self.input_num_capsule, 1, 1].\n",
        "        b = tf.zeros(shape=[tf.shape(inputs_hat)[0], self.n_capsule, \n",
        "                            self.input_n_capsule, 1, 1])\n",
        "\n",
        "        assert self.n_routing > 0, 'The routings should be > 0.'\n",
        "        for i in range(self.n_routing):\n",
        "            # Apply softmax to the axis with `num_capsule`\n",
        "            #  c.shape=[batch_size, num_capsule, input_num_capsule, 1, 1]\n",
        "            c = layers.Softmax(axis=1)(b)\n",
        "\n",
        "            # Compute the weighted sum of all the predicted output vectors.\n",
        "            #  c.shape =  [batch_size, num_capsule, input_num_capsule, 1, 1]\n",
        "            #  inputs_hat.shape=[None, num_capsule, input_num_capsule,dim_capsule,1]\n",
        "            # The function `multiply` will broadcast axis=3 in c to dim_capsule.\n",
        "            #  outputs.shape=[None, num_capsule, input_num_capsule, dim_capsule, 1]\n",
        "            # Then sum along the input_num_capsule\n",
        "            #  outputs.shape=[None, num_capsule, 1, dim_capsule, 1]\n",
        "            # Then apply squash along the dim_capsule\n",
        "            outputs = tf.multiply(c, inputs_hat)\n",
        "            outputs = tf.reduce_sum(outputs, axis=2, keepdims=True)\n",
        "            outputs = squash(outputs, axis=-2)  # [None, 10, 1, 16, 1]\n",
        "\n",
        "            if i < self.n_routing - 1:\n",
        "                # Update the prior b.\n",
        "                #  outputs.shape =  [None, num_capsule, 1, dim_capsule, 1]\n",
        "                #  inputs_hat.shape=[None,num_capsule,input_num_capsule,dim_capsule,1]\n",
        "                # Multiply the outputs with the weighted_inputs (inputs_hat) and add  \n",
        "                # it to the prior b.  \n",
        "                outputs_tiled = tf.tile(outputs, [1, 1, self.input_n_capsule, 1, 1])\n",
        "                agreement = tf.matmul(inputs_hat, outputs_tiled, transpose_a=True)\n",
        "                b = tf.add(b, agreement)\n",
        "\n",
        "        # End: Routing algorithm ------------------------------------------------#\n",
        "        # Squeeze the outputs to remove useless axis:\n",
        "        #  From  --> outputs.shape=[None, num_capsule, 1, dim_capsule, 1]\n",
        "        #  To    --> outputs.shape=[None, num_capsule,    dim_capsule]\n",
        "        outputs = tf.squeeze(outputs, [2, 4])\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        # output current layer capsules\n",
        "        return (None, self.n_capsule, self.n_vector)"
      ],
      "metadata": {
        "id": "T1_iMhArkiL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LengthLayer(Layer):\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return tf.sqrt(tf.reduce_sum(tf.square(inputs), axis=-1, keepdims=False))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        *output_shape, _ = input_shape\n",
        "        return tuple(output_shape)"
      ],
      "metadata": {
        "id": "uFptyQxVkmeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# probabilistic loss function used for classifying digits image.\n",
        "def margin_loss(y_ground_truth, y_prediction):\n",
        "    _m_plus = 0.9\n",
        "    _m_minus = 0.1\n",
        "    _lambda = 0.5\n",
        "    L = y_ground_truth * tf.square(tf.maximum(0., _m_plus - y_prediction)) + _lambda * ( 1 - y_ground_truth) * tf.square(tf.maximum(0., y_prediction - _m_minus))\n",
        "    return tf.reduce_mean(tf.reduce_sum(L, axis=1))"
      ],
      "metadata": {
        "id": "-V6DNSV9kpq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder network\n",
        "x = Input(shape=input_shape) # input for the encoder network\n",
        "conv1 = Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
        "primary_capsule = PrimaryCapsule(n_vector=8, n_channel=32, n_kernel_size=9, n_stride=2)(conv1)\n",
        "digit_capsule = DigitCapsuleLayer(n_capsule=n_class, n_vec=16, n_routing=n_routing, name='digit_capsule')(primary_capsule)\n",
        "output_capsule = LengthLayer(name='output_capsule')(digit_capsule)"
      ],
      "metadata": {
        "id": "ZbxtxoBFkre4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder network to reconstruct the input image\n",
        "mask_input = Input(shape=(n_class, )) # input for the digits capsule layer\n",
        "mask = MaskingLayer()([digit_capsule, mask_input])  # two inputs - the digit capsule and the masked input\n",
        "dec = Dense(512, activation='relu')(mask)\n",
        "dec = Dense(1024, activation='relu')(dec)\n",
        "dec = Dense(tot_n_pixels, activation='sigmoid')(dec) # reconstruct the input image\n",
        "dec = Reshape(input_shape)(dec)"
      ],
      "metadata": {
        "id": "wj_JA3Fck_bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "model = Model([x, mask_input], [output_capsule, dec])\n",
        "model.compile(optimizer='adam', loss=[ margin_loss, 'mae' ], metrics=[ margin_loss, 'mae', 'accuracy'])"
      ],
      "metadata": {
        "id": "hB_CPDcUlILZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "model.fit([X, Y], [Y, X], batch_size=128, epochs=3, validation_split=0.2)"
      ],
      "metadata": {
        "id": "gW9iYX8blLDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "label_predicted, image_predicted = model.predict([x_test, y_test])"
      ],
      "metadata": {
        "id": "Sy9G-VX4AKKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the predicted label to categorical values so that we can calculate the performance metrics\n",
        "pred_categorical = []\n",
        "actual_categorical = []  \n",
        "for i in range(0, len(label_predicted)):\n",
        "  pred_categorical.append(np.argmax(label_predicted[i])) \n",
        "  actual_categorical.append(np.argmax(y_test[i]))"
      ],
      "metadata": {
        "id": "kYqxpKpeAMQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, precision_score, recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(actual_categorical, pred_categorical)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(actual_categorical, pred_categorical, average='micro')\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(actual_categorical, pred_categorical, average='micro')\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(actual_categorical, pred_categorical, average='micro')\n",
        "print('F1 score: %f' % f1)\n",
        "# kappa\n",
        "kappa = cohen_kappa_score(actual_categorical, pred_categorical)\n",
        "print('Cohens kappa: %f' % kappa)"
      ],
      "metadata": {
        "id": "wMYzqAdSAO4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix\n",
        "matrix = confusion_matrix(actual_categorical, pred_categorical)\n",
        "print(matrix)"
      ],
      "metadata": {
        "id": "gZ_wPTSGARI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the predicted images\n",
        "\n",
        "n_samples = 5\n",
        "plt.figure(figsize=(n_samples * 2, 3))\n",
        "for index in range(n_samples):\n",
        "    plt.subplot(1, n_samples, index + 1)\n",
        "    sample_image = x_test[index].reshape(28, 28)\n",
        "    plt.imshow(sample_image, cmap=\"binary\")\n",
        "    plt.title(\"Label:\" + str(y_test[index]))\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(n_samples * 2, 3))\n",
        "for index in range(n_samples):\n",
        "    plt.subplot(1, n_samples, index + 1)\n",
        "    sample_image = image_predicted[index].reshape(28, 28)\n",
        "    plt.imshow(sample_image, cmap=\"binary\")\n",
        "    plt.title(\"Predicted:\" + str(np.argmax(label_predicted[index])))\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l8d2LYj5AUKt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}